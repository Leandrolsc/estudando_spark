# ğŸš€ LaboratÃ³rio de Estudos Spark com Docker

Este projeto oferece um ambiente de desenvolvimento Apache Spark completo e pronto para usar, totalmente containerizado com Docker. O objetivo Ã© simular um cluster real, permitindo que vocÃª estude, desenvolva e teste jobs de Big Data de forma isolada e prÃ¡tica no seu prÃ³prio computador.

## âœ¨ Principais Funcionalidades

* Ambiente Realista: Simula a arquitetura distribuÃ­da de um cluster real (Master/Workers), permitindo desenvolver cÃ³digo que Ã© diretamente portÃ¡vel para ambientes de produÃ§Ã£o como YARN ou Kubernetes.

* Desenvolvimento Interativo: Inclui um container com Jupyter Lab prÃ©-configurado para se conectar ao cluster, ideal para anÃ¡lise de dados e prototipaÃ§Ã£o.

* Simplicidade: Com apenas um comando (docker-compose up), todo o ambiente Ã© provisionado e fica pronto para uso.

* Isolado e PortÃ¡til: Evita a necessidade de instalar Java, Spark ou Python diretamente na sua mÃ¡quina.


## ğŸ—ï¸ Estrutura do Projeto

Para uma melhor organizaÃ§Ã£o, o projeto segue a estrutura abaixo. Certifique-se de criar as pastas ``jobs`` e ``data`` antes de iniciar.

```code
.
â”œâ”€â”€ ğŸ³ docker-compose.yml   # Orquestra todos os containers do ambiente.
â”œâ”€â”€ ğŸ“‚ jobs/                 # (Crie esta pasta) Onde seus scripts (.py) e notebooks (.ipynb) devem ficar.
â”œâ”€â”€ ğŸ“‚ data/                 # (Crie esta pasta) Local para armazenar datasets de entrada e saÃ­da.
â””â”€â”€ ğŸ“œ README.md             # Este arquivo de documentaÃ§Ã£o.
```

## ğŸ› ï¸ Como Executar o Ambiente

1. **PrÃ©-requisitos**
   - [Docker](https://www.docker.com/get-started) instalado
   - [Docker Compose](https://docs.docker.com/compose/) instalado

2. **Clone o repositÃ³rio**
   ```sh
   git clone https://github.com/leandrolsc/estudando_spark.git
   cd estudando_spark
   ```

3. **Crie as pastas necessÃ¡rias (se ainda nÃ£o existirem)**

Se as pastas ``jobs`` e ``data`` nÃ£o existirem, crie-as:

```sh
mkdir jobs data
```

4. **Inicie o Ambiente**

Execute o comando abaixo na raiz do projeto. O ``--build`` Ã© necessÃ¡rio apenas na primeira vez ou quando houver alteraÃ§Ãµes nos arquivos de configuraÃ§Ã£o.

```sh
docker-compose up --build -d
```

O ``-d`` (detached) executa os containers em segundo plano.

5. **Acesse os serviÃ§os**

ApÃ³s a inicializaÃ§Ã£o, os seguintes serviÃ§os estarÃ£o disponÃ­veis no seu navegador:

* Spark Master UI: http://localhost:8080 (Para monitorar o cluster e os workers)

* Jupyter Lab: http://localhost:8888 (Para desenvolver de forma interativa)


## ObservaÃ§Ãµes

* O ambiente Spark estÃ¡ configurado para nÃ£o exigir autenticaÃ§Ã£o, facilitando o uso local.
* Os dados e scripts sÃ£o persistidos nas pastas ``data/`` e ``jobs/`` do host.

---

Sinta-se Ã  vontade para adaptar o ambiente conforme suas necessidades de estudo ou desenvolvimento!