# Guia Pr√°tico: An√°lise de Dados com Spark no JupyterLab

Parab√©ns! Seu ambiente interativo est√° no ar.  
Este notebook √© um guia de ponta a ponta sobre como usar o Spark para carregar, transformar e consultar dados.

---

## Passo 1: A Funda√ß√£o - A SparkSession

Tudo no Spark come√ßa com a **SparkSession**.  
Ela √© o seu ponto de entrada para se conectar ao cluster Spark, configurar o ambiente e come√ßar a trabalhar.  

Execute esta c√©lula primeiro. Ela prepara tudo para n√≥s.

```python
# Importa a biblioteca necess√°ria
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, desc, year

# 1. Cria e configura a SparkSession
spark = SparkSession.builder \
    .appName("AnaliseInterativaClima") \
    .master("spark://spark-master:7077") \
    .config("spark.sql.warehouse.dir", "/opt/bitnami/spark/spark-warehouse") \
    .enableHiveSupport() \
    .getOrCreate()

# O que cada linha faz:
# .appName("..."): D√° um nome para sua aplica√ß√£o, que aparecer√° na UI do Spark (localhost:8080).
# .master("..."): Aponta para o nosso cluster Spark no Docker. √â aqui que a m√°gica da conex√£o acontece.
# .config("..."): Informa ao Spark onde ele deve guardar os dados das tabelas que criarmos.
# .enableHiveSupport(): Ativa o cat√°logo de tabelas, permitindo usar SQL de forma mais poderosa.

print("SparkSession criada com sucesso!")
print("Pode come√ßar a an√°lise. Use a vari√°vel 'spark' para todas as opera√ß√µes.")
print("Acesse a UI do Spark para ver sua aplica√ß√£o rodando: http://localhost:8080")
```

## Passo 2: Carregando Dados

Vamos carregar um arquivo parquet para dentro de um DataFrame.
Pense em um DataFrame como uma tabela do Excel ou de um banco de dados, mas distribu√≠da em um cluster e com superpoderes.

üìå Obs: Utilize o arquivo parquet que est√° em data/input/clima_inmet_aggregated.parquet.


Carregando o parquet:

```python
# O caminho √© o mesmo que est√° dentro do container
caminho_arquivo = "data/input/clima_inmet_aggregated.parquet"

# Lendo o arquivo parquet e criando um DataFrame
df_clima = spark.read.parquet(
            "data/input/clima_inmet_aggregated.parquet",
            header=True,
            inferSchema=True
        )

# Vamos inspecionar os dados para ver se carregou corretamente
print("Esquema do DataFrame (tipos de dados das colunas):")
df_clima.printSchema()

print("\nAmostra dos dados (primeiras 5 linhas):")
df_clima.show(5)

```

## Passo 3: Fazendo Consultas - 2 Maneiras Principais

Existem duas formas populares de fazer consultas no Spark:

* API de DataFrames (program√°tica)

* Spark SQL (SQL puro)

### M√©todo 1: API de DataFrames

Pergunta de neg√≥cio:
"Quais as cidades brasileiras e seus respectivos estados com maior temperatura m√©dia, ordene do maior para o menor?"

```python
# Usando a API de DataFrames para responder √† pergunta
med_cidades = df_clima.groupBy("Cidade", "UF") \
    .agg(round(avg("Med_Temperatura"), 2).alias("Temperatura_M√©dia")) \
    .orderBy(desc("Temperatura_M√©dia"))

print("Cidades Brasileiras Temperaturas m√©dias nos ultimos 15 anos:")
med_cidades.show()
```


### M√©todo 2: Spark SQL

Se voc√™ vem de um mundo de bancos de dados, vai se sentir em casa.
Para usar SQL, primeiro precisamos registrar o DataFrame como uma view tempor√°ria.

Mesma pergunta de neg√≥cio:
"Quais as cidades brasileiras e seus respectivos estados com maior temperatura m√©dia, ordene do maior para o menor?"

```python
# 1. Registra o DataFrame como uma view tempor√°ria
df_clima.createOrReplaceTempView("clima_view")

# 2. Agora, execute uma consulta SQL pura!
sql_query = """
    SELECT
        Cidade,
        UF,
        AVG(Med_Temperatura) AS Temperatura_M√©dia
    FROM
        clima_view
    GROUP BY
        Cidade,
        UF,
    ORDER BY
        Temperatura_M√©dia DESC
"""

clima_medio_df = spark.sql(sql_query)

print("Resultado da consulta com Spark SQL:")
clima_medio_df.show()
```


## Passo 4: Criando Tabelas Permanentes

Views s√£o tempor√°rias e s√≥ existem na sess√£o atual.
Se voc√™ quiser que suas tabelas persistam mesmo ap√≥s reiniciar o notebook, pode salv√°-las no warehouse.

```python
# Salvando o DataFrame original como uma tabela gerenciada pelo Spark
df_clima.write.mode("overwrite").saveAsTable("tabela_clima_permanente")

# 'mode("overwrite")' significa que se a tabela j√° existir, ela ser√° substitu√≠da.

print("Tabela criada com sucesso!")

# Agora vamos provar que ela existe no cat√°logo do Spark
spark.sql("SHOW TABLES").show()
```

Agora, em qualquer outro notebook (ou no mesmo, depois de reiniciar), voc√™ pode pular o passo de ler o parquet e consultar a tabela diretamente:

```python
# Lendo diretamente da tabela que acabamos de criar
df_direto_da_tabela = spark.sql("SELECT * FROM tabela_clima_permanente WHERE cidade = 'Belo Horizonte'")

df_direto_da_tabela.show()
```

## Passo 5: Salvando os Resultados

Ap√≥s sua an√°lise, voc√™ provavelmente vai querer salvar o resultado em um arquivo.
O formato Parquet √© altamente recomendado por ser otimizado para performance e compress√£o.

```python
# Vamos salvar o resultado da nossa agrega√ß√£o (Clima de Belo Horizonte)
caminho_saida = "/opt/bitnami/spark/data/output/clima_agregado"

df_direto_da_tabela.write.mode("overwrite").parquet(caminho_saida)

print(f"Resultado salvo com sucesso em '{caminho_saida}' na sua pasta 'data/output'!")
```


---

## üìå Boas Pr√°ticas com Spark

1. Use Particionamento ao Salvar Dados

Sempre que poss√≠vel, utilize .partitionBy("coluna") ao salvar datasets grandes. Isso melhora a leitura e filtragem posterior.

```python
df_clima.write.partitionBy("Cidade").parquet(caminho_saida)
```

2. Aproveite o Cache para Consultas Repetidas

Se voc√™ vai reutilizar um DataFrame v√°rias vezes em consultas diferentes, use .cache() ou .persist().

```python
df_clima.cache()
```

3. Evite collect() em grandes datasets

O m√©todo collect() traz todos os dados para o driver. Prefira show() ou opera√ß√µes agregadas.

4. Leia o Plano de Execu√ß√£o

Use .explain() para entender como o Spark vai processar sua query e identificar gargalos.

```python
df_clima.explain()
```

5. Prefira Formatos Otimizados (Parquet/Delta)

Parquet ou Delta s√£o mais r√°pidos, comprimidos e otimizados para an√°lises.

6. Monitore na UI do Spark

Sempre acompanhe o processamento pelo painel em http://localhost:8080. Ele ajuda a identificar stages lentos ou mal otimizados.
