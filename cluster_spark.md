# ‚ú® Entendendo um Cluster Spark
Um cluster Spark √©, em ess√™ncia, um grupo de computadores trabalhando em conjunto para processar grandes volumes de dados de forma muito mais r√°pida do que um √∫nico computador conseguiria. √â a base do processamento de Big Data e o que torna o Spark t√£o poderoso.

A ideia principal √© o **processamento paralelo**: em vez de uma √∫nica m√°quina ler um arquivo de 100GB linha por linha, um cluster Spark divide esse arquivo em 100 peda√ßos de 1GB e coloca 100 m√°quinas para trabalhar, cada uma em seu pr√≥prio peda√ßo, ao mesmo tempo.

## üèõÔ∏è A Arquitetura de um Cluster
Um cluster Spark Standalone (como o que usamos no Docker) possui dois componentes principais:

1. N√≥ Mestre (Master Node): √â o "c√©rebro" ou o "gerente" do cluster. Ele n√£o processa os dados, mas coordena todo o trabalho. Sua fun√ß√£o √© saber quais m√°quinas est√£o dispon√≠veis e distribuir as tarefas entre elas. No nosso ``docker-compose.yml``, este √© o servi√ßo ``spark-master``.

2. N√≥s de Trabalho (Worker Nodes): S√£o os "trabalhadores" do cluster. S√£o as m√°quinas que efetivamente executam as tarefas de processamento de dados. Eles recebem ordens do Master e reportam o status de volta. Em nosso ``docker-compose.yml``, s√£o os servi√ßos ``spark-worker`` e ``spark-worker-2``.

## ‚öôÔ∏è Como um Job √© Executado no Cluster? (O Fluxo de Trabalho)
Entender este fluxo √© crucial para depurar e otimizar seus jobs. Vamos ver o que acontece passo a passo quando voc√™ executa o comando ``spark-submit`` ou uma c√©lula no Jupyter.

#### Componentes em Jogo:

* **Driver Program:** √â o seu processo principal, onde a ``SparkSession`` √© criada. Ele roda no Jupyter ou no terminal de onde voc√™ lan√ßou o ``spark-submit``.

* **Cluster Manager (Master):** O servi√ßo ``spark-master`` que gerencia os recursos.

* **Executors:** Processos especiais que s√£o lan√ßados nos Worker Nodes para executar as tarefas.

#### O Fluxo Detalhado:

1. **In√≠cio (Seu C√≥digo):** Voc√™ executa seu script. A primeira coisa que seu c√≥digo faz √© criar uma ``SparkSession``. Neste momento, nasce o Driver Program.

2. **Pedido de Recursos:** O Driver Program se conecta ao Master Node (usando o endere√ßo ``spark://spark-master:7077``). Ele diz: "Ol√°, sou uma nova aplica√ß√£o e preciso de X cores de CPU e Y de mem√≥ria para executar meu trabalho".

3. **Aloca√ß√£o:** O Master, que conhece todos os Workers dispon√≠veis, responde ao Driver alocando os recursos. Ele comanda os Worker Nodes para criarem processos Executor. Um Executor √© como um "espa√ßo de trabalho" em um Worker, com uma quantidade reservada de CPU e mem√≥ria.

4. **Distribui√ß√£o de Tarefas:** Agora, o Driver analisa seu c√≥digo (ex: ``df.filter(...).groupBy(...)``). Ele o quebra em pequenas unidades de trabalho chamadas Tasks e envia essas tasks diretamente para os Executors nos diferentes Worker Nodes.

5. **Execu√ß√£o Paralela:** Cada Executor processa suas tasks em paralelo, trabalhando em uma parti√ß√£o diferente dos dados. √â aqui que a m√°gica da velocidade acontece.

6. **Retorno do Resultado:** Conforme os Executors terminam suas tasks, eles podem enviar os resultados de volta para o Driver (se voc√™ usar uma a√ß√£o como ``.show()`` ou ``.collect()``) ou salvar o resultado final em um sistema de arquivos (com ``.write.parquet()``).

## üõ†Ô∏è Gerenciando Recursos com ``docker-compose.yml``
Seu arquivo ``docker-compose.yml`` √© o painel de controle para configurar os recursos do seu cluster local. Voc√™ pode simular m√°quinas mais fracas ou mais fortes alterando algumas vari√°veis de ambiente nos servi√ßos dos workers.

Vamos analisar o servi√ßo ``spark-worker`` como exemplo:

```yml
# docker-compose.yml

  spark-worker:
    build:
      context: ./spark
    container_name: spark-worker-1
    depends_on:
      - spark-master
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      # --- LINHAS IMPORTANTES PARA GERENCIAR RECURSOS ---
      - SPARK_WORKER_MEMORY=1G   # <-- Altere aqui a mem√≥ria RAM do worker
      - SPARK_WORKER_CORES=1     # <-- Altere aqui o n√∫mero de CPUs do worker
      # --------------------------------------------------
    volumes:
      - ./jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data
```
#### Aumentando CPU e Mem√≥ria
Digamos que voc√™ queira simular um worker mais potente. Voc√™ pode simplesmente editar essas linhas:

* ``SPARK_WORKER_CORES=2``: Agora este worker dir√° ao Master que ele pode executar 2 tarefas simultaneamente.

* ``SPARK_WORKER_MEMORY=4G``: Agora este worker ter√° 4 Gigabytes de RAM dispon√≠veis para seus executors.

**Exemplo de um worker "turbinado":**

```yml
  spark-worker-turbinado:
    build:
      context: ./spark
    container_name: spark-worker-3 # Novo nome
    ports:
      - "8083:8081" # Nova porta para n√£o haver conflito
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G   # <-- 4GB de RAM
      - SPARK_WORKER_CORES=2     # <-- 2 n√∫cleos de CPU
    # ... resto da configura√ß√£o
```
#### Escalando o Cluster (Adicionando mais Workers)
Para aumentar o poder de processamento total, voc√™ pode adicionar mais workers. A maneira mais f√°cil √© copiar e colar o bloco de um servi√ßo spark-worker existente e alterar duas coisas para evitar conflitos:

1. O nome do servi√ßo (ex: ``spark-worker-3``).

2. O ``container_name``.

3. A porta mapeada no host (ex: ``"8083:8081"``).

Lembrete Importante: Sempre que voc√™ fizer altera√ß√µes no seu arquivo ``docker-compose.yml``, precisa parar e recriar os containers para que as mudan√ßas tenham efeito:

```sh
# Parar e remover os containers antigos
docker-compose down

# Subir o ambiente com as novas configura√ß√µes
docker-compose up -d
```
